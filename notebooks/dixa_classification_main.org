#+TITLE:   Classification of DiXa arrays
#+AUTHOR:  Jumamurat Bayjanov
#+EMAIL:   jumamurat@gmail.com
#+STARTUP: indent
#+STARTUP: overview
#+STARTUP: align
#+LATEX_COMPILER: pdflatex

* Prepare names of studies that needs to be downloaded from DiXa
** Retrieve study names based on the following criteria
*** Data location info and selection criteria 
- URL: http://wwwdev.ebi.ac.uk/fg/dixa/browsestudies.html
- Ignore the following projects: New-Generis, Envirogenomarkers
- In vitro study. Ignore in-vivo, but include ex-vivo studies
- Array data is available for a given study
- Ignore miRNA and protein arrays, So include only mRNA arrays
- Discard this sample: NTC_WP4.99.1_E01 (from NTC project), for now ignore all NTC projects
- Ignore all "Drug Matrix" projects, except the DIXA-033
- Ignore all "PredTox" projects, as they are all in-vivo experiments
- Retrieve data from this folder: ftp://ftp.ebi.ac.uk/pub/databases/microarray/data/dixa/
- TG-GATES data is already available on our servers, but we decided to download from Dixa DB: project id DIXA-006

*** Code to retrieve study info: Retrieval type 1
This version retrieves data automatically, but data sets are so diverse and very cumbersome.
So we decided to download it from the provided URLs. See Retrieval type 2.

:PROPERTIES:
:header-args: :session ses1
:END:

#+NAME: activate_env
#+BEGIN_SRC shell :results output :session ses1 :eval no
#source activate dixa_classification
source /home/bayjan/.bashrc
export PATH="/home/bayjan/tools/miniconda3/bin:$PATH"
source deactivate
source activate dixa_classification
python -c 'from selenium import webdriver'
python -c 'print("works")'
echo "Currently activating virtual env does NOT work. So start emacs from the active virtual env, \
where you want to run your python commands. Thus, currently eval header is set to no"
#+END_SRC

#+NAME: prepare_urls_to_download
#+BEGIN_SRC python :results output :noweb yes 
# Do tangle it for now :tangle ../src/prepare_urls_to_download.py
# Import needed modules
from selenium import webdriver
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.support.ui import WebDriverWait # available since 2.4.0
from selenium.webdriver.support import expected_conditions as EC # available since 2.26.0

import re

<<get_good_accessions>>

<<experiments_info>>

<<main>>
#+END_SRC


#+NAME: get_good_accessions
#+BEGIN_SRC python  :results output :noweb yes :tangle test1.py 

def get_good_accessions(url):
    # I had already installed phantomjs and it is in the PATH
    driver = webdriver.PhantomJS()
    driver.get(url)
    print("URL=" + url)
    accession_list = driver.find_elements_by_class_name('col_accession')
    projects = driver.find_elements_by_class_name('col_project')
    
    accessions_to_parse=[]
    ignore_projects =  ['new-generis','envirogenomarkers', 'ntc','predtox']
    for i,proj in enumerate(projects):
        if not proj.text.lower() in ignore_projects:
            accessions_to_parse.append(accession_list[i].text)

    driver.close()
    return(accessions_to_parse)



#+END_SRC

#+NAME: main 
#+BEGIN_SRC python :noweb yes
if __name__ == "__main__":
    <<call_get_accessories>>

#+END_SRC

#+NAME: experiments_info
#+BEGIN_SRC python :noweb yes
def experiments_info(accession_ids, url_prefix='http://wwwdev.ebi.ac.uk/fg/dixa/group/', url_suffix='?keywords', technology_types=['array'], descr_in=['vitro'], descr_out=['vivo']):
    '''
    This function retrieves information about possibly further used and ignored experiment info.
    @param descr_in: words that should      be in the desription of the experiment
    @param descr_out: words that should NOT be in the desription of the experiment
    '''
    driver = webdriver.PhantomJS()
    # The next dict is of the form: {'acc_id':{'title1':'content1',...,'titleN':'contentN'}}
    experiments_info = {}
    ignored_experiments = {}
    for id in accession_ids:
        good_experiment = True
        url = url_prefix + id + url_suffix
        driver.get(url)
        titles = driver.find_elements_by_class_name('col_title')
        # contents = driver.find_elements_by_class_name('col_contents')
        # titles and contents should have the exactly same number of elements
        # assert len(titles) == len(contents)
        description = ''
        tech_type = ''
        contents_dict = {}
        for i, t in enumerate(titles):
            title = t.text.lower()
            title1 = re.sub('\s*:$','',t.text) # This one we need for Xpath search
            retrieved_data = driver.find_elements_by_xpath('//div[@class="col_title" and contains(text(),"' + title1 + '")]/ancestor::td/following-sibling::td/div[@class="col_contents"]/*')
            if len(retrieved_data) == 0:
                # Then re-retrieve it, because it is not an array but just a text
                retrieved_data = driver.find_elements_by_xpath('//div[@class="col_title" and contains(text(),"' + title1 + '")]/ancestor::td/following-sibling::td/div[@class="col_contents"]')
                assert len(retrieved_data) == 1 # There should be only single entry
                contents_dict[title1] = retrieved_data[0].text
            else:
                contents_dict[title1] = [rd.text for rd in retrieved_data]
                contents_dict[title1] = ';'.join(contents_dict[title1])
            # The next if statement checks for description field
            if title.count('description') > 0:
                # Check in the description for descr_in and descr_out conditions
                descr = contents_dict[title1].lower()
                description = descr
                # The words in descr_in should be present
                # BUT we do NOT check for this for now, because description is very sloppily written
                # for d in descr_in:
                #    if descr.count(d.lower()) < 1:
                #        good_experiment = False
                # The words in descr_out should NOT be present
                # SO, we only check if unwanted term is present or not
                # Apparently this is also not a good check, so I comment this one as well for now
                # for d in descr_out:
                #    if descr.count(d.lower()) > 0:
                #        good_experiment = False
                if descr.count('vivo')>0 and descr.count('vitro')<1:
                    # Then this is an in-vivo experiment, so discard it
                    # Also check for cases with "Ex vivo" or "Ex-vivo"
                    if len(re.findall('ex[\s-]*vivo',s, re.IGNORECASE)) < 1:
                        good_experiment = False
                    
            if title.count('technology') > 0 and title.count('type') > 0:
                tech_type = contents_dict[title1].lower()
                # Then this is "Technology Type:" row
                # The words in technology_types Must be present                    
                for tech in technology_types:
                    if contents_dict[title1].lower().count(tech.lower()) < 1:
                        good_experiment = False
        if good_experiment:
            # Then add this experiment's info to a dictionary
            # Remove unnesseray ' :' suffix from titles
            # experiments_info[id]={re.sub('\s*:$','',titles[k].text):contents[k].text for k in range(len(titles))}
            experiments_info[id] = contents_dict
        else:
            print("The following experiment has either a problem in description or technology type")
            print(id)
            print("description:=", descr)
            print("tech type:=", tech_type)
            # ignored_experiments[id] = {titles[k].text:contents[k].text for k in range(len(titles))}
            ignored_experiments[id] = contents_dict
    driver.close()
    print("In total {} out of {} are possibly good experiments".format(len(experiments_info.keys()), len(accession_ids)))
    return(experiments_info, ignored_experiments)

#+END_SRC

#+NAME: call_get_accessories
#+BEGIN_SRC python :results output :noweb yes
url = 'http://wwwdev.ebi.ac.uk/fg/dixa/group/browse-table-studies.html?keywords=&sortby=relevance&sortorder=descending&page=1&pagesize=100'
accessions2 = get_good_accessions(url)
used_experiments, ignored_experiments = experiments_info(accession_ids=accessions2)

# Just to test
import json
with open('../data/interim/used_experiments_all1.json','w') as out:
    json.dump(used_experiments,out)
with open('../data/interim/ignored_experiments_all1.json','w') as out:
    json.dump(ignored_experiments,out)

#+END_SRC

#+NAME: run_py_code_in_shell
#+HEADERS: :var py_code="../src/prepare_urls_to_download.py"
#+BEGIN_SRC shell :results output
source deactivate
source activate dixa_classification
python ${py_code}
#+END_SRC


#+NAME:  temporary_py_code
#+BEGIN_SRC python :eval no
# Temporary py code
cmd1=driver.find_elements_by_xpath('//div[@class="col_title" and contains(text(),"ompound")]/ancestor::td/following-sibling::td/div[@class="col_contents"]//div[contains(@class,"label")]')
#+END_SRC

*** Retrieve data from provided URLs: Retrieval type 2
This version retrieves data from provided URLs
- URL: ftp://ftp.ebi.ac.uk/pub/databases/microarray/data/dixa/
- There are many projects in this ftp location, but bring the following projects only
  - Carcinogenomics: all in-vitro
  - Drug Matrix: only study  DIXA-033
  - Esnats: all in-vitro

#+NAME: download_array_data1
#+HEADERS: :var data_dir="/home/jbayjanov/projects/tgx/dixa_classification/data/raw/" :prologue "#!/bin/bash" :eval no
#+BEGIN_SRC shell :noweb yes :tangle  /home/bayjan/projects/tgx/dixa_classification/src/download_data.sh
set -o errexit
set -o nounset
set -o pipefail
echo "You should download these arrays to NGS-ada machine, because these files are too big"
cd ${data_dir}
mkdir -pv carcinogenomics
cd carcinogenomics
echo "First downloading array info files"
wget ftp://ftp.ebi.ac.uk/pub/databases/microarray/data/dixa/CarcinoGenomics/archive/a_kidney_transcription%20profiling_DNA%20microarray.txt
wget ftp://ftp.ebi.ac.uk/pub/databases/microarray/data/dixa/CarcinoGenomics/archive/a_lung_transcription%20profiling_DNA%20microarray.txt
wget ftp://ftp.ebi.ac.uk/pub/databases/microarray/data/dixa/CarcinoGenomics/archive/liver/a_liver_transcription%20profiling_DNA%20microarray.txt

echo "Downloading study info files"
wget ftp://ftp.ebi.ac.uk/pub/databases/microarray/data/dixa/CarcinoGenomics/archive/s_Kidney.txt
wget ftp://ftp.ebi.ac.uk/pub/databases/microarray/data/dixa/CarcinoGenomics/archive/s_Liver.txt
wget ftp://ftp.ebi.ac.uk/pub/databases/microarray/data/dixa/CarcinoGenomics/archive/s_Lung.txt

# Also get NMR data info file, because it is needed to filter out files that are in this file
wget ftp://ftp.ebi.ac.uk/pub/databases/microarray/data/dixa/CarcinoGenomics/archive/liver/a_liver_metabolite%20profiling_NMR%20spectroscopy.txt
# Get rid of spaces in file names
rename 's/\s+/_/g' *.txt

<<download_carcinogenomics_arrays>>

<<download_drugmatrix_dixa033>>

<<download_esnats>>

<<download_tk6_data>>

<<download_hepg2_data>>

#+END_SRC

#+NAME: download_carcinogenomics_arrays
#+HEADERS: :var data_dir="/home/jbayjanov/projects/tgx/dixa_classification/data/raw/"  :eval no
#+BEGIN_SRC shell :noweb yes
mkdir carcinogenomics
cd carcinogenomics
wget --continue -m ftp://ftp.ebi.ac.uk/pub/databases/microarray/data/dixa/CarcinoGenomics/archive/kidney/micro/
wget --continue -m ftp://ftp.ebi.ac.uk/pub/databases/microarray/data/dixa/CarcinoGenomics/archive/liver/micro/
wget --continue -m ftp://ftp.ebi.ac.uk/pub/databases/microarray/data/dixa/CarcinoGenomics/archive/lung/micro/
mv ./ftp.ebi.ac.uk/pub/databases/microarray/data/dixa/CarcinoGenomics/archive/* .
rm -rf ./ftp.ebi.ac.uk/
# This is the local data
rsync -auvz /ngs-data/data_storage/transcriptomics/microarray/mrna/Carcinogenomics_extra .
cd Carcinogenomics_extra
# Remove array platform info from file names
rename 's/_\([^\)]+\)//' *.CEL
#+END_SRC

#+NAME: download_drugmatrix_dixa033
#+HEADERS: :var data_dir="/home/jbayjanov/projects/tgx/dixa_classification/data/raw/"  :eval no
#+BEGIN_SRC shell
echo "From DrugMatrix experiment we only download a single study dixa033"
cd ${data_dir}
mkdir -pv drugmatrix
cd drugmatrix
wget --continue -m ftp://ftp.ebi.ac.uk/pub/databases/microarray/data/dixa/DrugMatrix/archive/hepatocyte/
mv ./ftp.ebi.ac.uk/pub/databases/microarray/data/dixa/DrugMatrix/archive/hepatocyte .
rm -rf ./ftp.ebi.ac.uk/
#+END_SRC

#+NAME: download_esnats
#+HEADERS: :var data_dir="/home/jbayjanov/projects/tgx/dixa_classification/data/raw/" :eval no
#+BEGIN_SRC shell
cd ${data_dir}
mkdir -pv esnats
cd esnats
# NO NEED for e_mexp_3577 study, because it is NOT about liver
#echo "esnats project has 2 studies and we will download them each separately, because they are located at different locations"
#mkdir -pv e_mexp_3577
#cd e_mexp_3577
#wget https://www.ebi.ac.uk/arrayexpress/files/E-MEXP-3577/E-MEXP-3577.processed.1.zip
#wget https://www.ebi.ac.uk/arrayexpress/files/E-MEXP-3577/E-MEXP-3577.raw.1.zip
#wget https://www.ebi.ac.uk/arrayexpress/files/E-MEXP-3577/E-MEXP-3577.idf.txt
#wget https://www.ebi.ac.uk/arrayexpress/files/E-MEXP-3577/E-MEXP-3577.sdrf.txt
#cd ../
wget --continue -m ftp://ftp.ebi.ac.uk/pub/databases/microarray/data/dixa/Esnats/archive/UKK4_archive
mv ./ftp.ebi.ac.uk/pub/databases/microarray/data/dixa/Esnats/archive/UKK4_archive ukk4_archive
rm -rf ./ftp.ebi.ac.uk/
#+END_SRC

#+NAME:  download_tk6_data
#+HEADERS: :var data_dir="/home/jbayjanov/projects/tgx/dixa_classification/data/raw/"  :eval no
#+BEGIN_SRC shell
cd ${data_dir}
mkdir -pv tk6
cd tk6
echo "Downloading TK6 data from the following projects"
echo "https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE58431"
wget -O GSE58431_RAW.tar "https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE58431&format=file"
echo "https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE51175"
wget -O GSE51175_RAW.tar "https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE51175&format=file"
# We will probably be using Normalized data, so download them as well
tar -xvf *.tar
cd GSE58431_RAW
wget ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE58nnn/GSE58431/suppl/GSE58431%5FNormalized%5Fdata%2Etxt%2Egz
gunzip GSE58431_Normalized_data.txt.gz
## Downloading GSE51175 normalized was rather hectic, because each array was separately stored at GEO database
cd GSE51175_RAW
mkdir normalized
cd normalized
wget ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE51nnn/GSE51175/matrix/GSE51175_series_matrix.txt.gz
gunzip GSE51175_series_matrix.txt.gz
grep sample_id GSE51175_series_matrix.txt |cut -f2-|tr -d '"' |tr ' ' '\n'|grep -Ev '^\s*$' > GSE51175_GSM_array_ids.txt
for i in $(cat GSE51175_GSM_array_ids.txt); do echo "curl -L -X GET \"https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?view=data&acc=${i}\""; curl -L -X GET "https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?view=data&acc=${i}"|awk 'BEGIN{p=0;}{if(index($0,"</pre>")){p=0;}; if(p==1){print($0)}; if(index($0,"<pre>")>0){p=1;}}'|sed -r 's/<[^>]+>//g'|grep -Ev '^\s*$'  > GSE51175_${i}_normalized.txt; done >> GSE51175_arrays_download_links.txt
#+END_SRC

#+NAME: download_hepg2_data
#+HEADERS: :var data_dir="/home/jbayjanov/projects/tgx/dixa_classification/data/raw/" :eval no
#+BEGIN_SRC shell
cd ${data_dir}
mkdir -pv hepg2
cd hepg2
#echo "Downloading HepG2 data from https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE28878"
#wget -O GSE28878_RAW.tar "https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE28878&format=file"
# /ngs-data/data_storage/transcriptomics/microarray/mrna/STW/Magkoufopoulou/CM_Hepg2_GTX_NTGX_560A_archive
# WARNING!! I deleted the data I had downloaded from the web, because we already have it in-house.
# So, I will use the in-house version. I created the symlink to the in-house version
#+END_SRC

#+NAME: download_predictomics_data
#+HEADERS: :eval no  :var data_dir="/home/jbayjanov/projects/tgx/dixa_classification/data/raw/" 
#+BEGIN_SRC shell
cd ${data_dir}
mkdir predictomics
cd predictomics
wget -m ftp://ftp.ebi.ac.uk/pub/databases/microarray/data/dixa/Predictomics/archive/Predictomics/
mv ./ftp.ebi.ac.uk/pub/databases/microarray/data/dixa/Predictomics/archive/Predictomics/* .
rm -rf ./ftp.ebi.ac.uk/
#+END_SRC

#+NAME: download_tg_gates_dixa006
#+HEADERS: :eval no  :var data_dir="/home/jbayjanov/projects/tgx/dixa_classification/data/raw/" 
#+BEGIN_SRC shell
cd ${data_dir}
mkdir tg_gates
cd tg_gates
wget --mirror ftp://ftp.ebi.ac.uk/pub/databases/microarray/data/dixa/TG-Gates/archive/DIXA-006/
#+END_SRC

#+NAME: link_to_ketelslegers_data
#+HEADERS: :eval no  :var data_dir="/home/jbayjanov/projects/tgx/dixa_classification/data/raw/"
#+BEGIN_SRC shell
cd ${data_dir}
mkdir ketelslegers
cd ketelslegers
ln -s /ngs-data/data_storage/transcriptomics/microarray/mrna/Aanjaag/Ketelslegers .
#+END_SRC

*** DONE Copy all of this project to ngs-ada machine and run the retrieval type 2 there

** TODO Create a list of compounds studied in the selected work
- Search for s_* files in the downloaded folders and analyze those files for compound names
- Currently ignore this, not a high priority
- Send list of ChemblID:Name to Danyel

#+NAME: parse_compounds_info
#+HEADERS: :var data_dir="/home/jbayjanov/projects/tgx/dixa_classification/data/raw/" :prologue "#!/bin/bash" :eval no
#+BEGIN_SRC shell :noweb yes :tangle  /home/bayjan/projects/tgx/dixa_classification/src/parse_compounds_info.sh
set -o errexit
set -o nounset
set -o pipefail
echo "You should download these arrays to NGS-ada machine, because these files are too big"
cd ${data_dir}
cd carcinogenomics
echo "First calculating compound chembl codes"
chembl_col=`awk 'BEGIN{FS="\t"} NR==1{for(i=1; i<=NF; i++) if(tolower($i) ~ /chembl/) {print(i)}}' s_Liver.txt`
awk 'NR>1 {print}' s_Liver.txt|cut -f $chembl_col  |tr -d '"'|grep -Ev '^\s*$'|sort -V|uniq > compound_codes_Liver.txt
# Column 24 contains compound names
while read i; do c=`grep -E $i s_Liver.txt |cut -f24|tr -d '"'|sort|uniq`; echo -e "$c\t$i"; done < compound_codes_Liver.txt |sort -k1,2 > compound_names_codes_Liver.txt
#+END_SRC


** Normalize all array data
*** DONE Normalize carcinogenomics arrays
- We will only use human arrays
- We will only use liver array data
#+NAME: prepare_carcinogenomics_array_files
#+HEADERS: :var data_dir="/home/jbayjanov/projects/tgx/dixa_classification/data/raw/carcinogenomics/" :prologue "#!/bin/bash" :eval no
#+BEGIN_SRC shell :noweb yes :tangle /home/bayjan/projects/tgx/dixa_classification/src/normalize_carcinogenomics_data.sh
cd ${data_dir}
awk -F $'\t' '$1 ~ /R/ && $3 !~ /HepG2\//{print}' s_Liver.txt|cut -f3 |tr -d '"'|sort|uniq -c
#   314 HepaRG
#   211 HepG2
#   145 HepG2-up
#   188 hESC_DE-Hep
#   205 Primary rat hepatocytes non TSA-treated
#     6 Primary rat hepatocytes TSA-non-treated
#     6 Primary rat hepatocytes TSA treated 
#   196 Primary rat hepatocytes TSA-treated
# I tested the above awk command and some of them are mass-spec data, 
# which is tested again by the following command
a=`join -j 1 <(awk -F $'\t' '$1 ~ /R/ && $1 !~ /_W_/ && $3 !~ /HepG2\//{print}' s_Liver.txt|cut -f1|sort) <(sort a_liver_metabolite_profiling_NMR_spectroscopy.txt )|wc -l` # a -> 0
awk -F $'\t' '$1 ~ /R/ && $1 !~ /_W_/ && $3 !~ /HepG2\//{print}' s_Liver.txt|cut -f3|sort|uniq -c
#    314 "HepaRG"
#    195 "HepG2"
#    145 "HepG2-up"
#    188 "hESC_DE-Hep"
#    205 "Primary rat hepatocytes non TSA-treated"
#    196 "Primary rat hepatocytes TSA-treated"
# So these 6 cell models are present for liver microarray data and for each of them normalization need to be done separately.
# The next line gives 314-line output
join -j 1 <(awk -F $'\t' '$1 ~ /R/ && $1 !~ /_W_/ && $3 !~ /HepG2\// && $3 ~ /"HepaRG"/ {print}' s_Liver.txt|cut -f1|tr -d '"'|sed -re 's/\s+/_/g'|sort) <(ls liver/micro/|sed -re 's/\.CEL//g'|sort)|sed -re 's/$/.CEL/g'  > liver_heparg_files.txt
join -j 1 <(awk -F $'\t' '$1 ~ /R/ && $1 !~ /_W_/ && $3 !~ /HepG2\// && $3 ~ /"HepG2"/ {print}' s_Liver.txt|cut -f1|tr -d '"'|sed -re 's/\s+/_/g'|sort) <(ls liver/micro/|sed -re 's/\.CEL//g'|sort)|sed -re 's/$/.CEL/g' > liver_hepG2_files.txt
join -j 1 <(awk -F $'\t' '$1 ~ /R/ && $1 !~ /_W_/ && $3 !~ /HepG2\// && $3 ~ /"HepG2-up"/ {print}' s_Liver.txt|cut -f1|tr -d '"'|sed -re 's/\s+/_/g'|sort) <(ls liver/micro/|sed -re 's/\.CEL//g'|sort)|sed -re 's/$/.CEL/g' > liver_hepG2up_files.txt
join -j 1 <(awk -F $'\t' '$1 ~ /R/ && $1 !~ /_W_/ && $3 !~ /HepG2\// && $3 ~ /"hESC_DE-Hep"/ {print}' s_Liver.txt|cut -f1|tr -d '"'|sed -re 's/\s+/_/g'|sort) <(ls liver/micro/|sed -re 's/\.CEL//g'|sort)|sed -re 's/$/.CEL/g' > liver_hESC_DE_Hep_files.txt
# The next two are rat cell models, so we are not going to use it now.
join -j 1 <(awk -F $'\t' '$1 ~ /R/ && $1 !~ /_W_/ && $3 !~ /HepG2\// && $3 ~ /"Primary rat hepatocytes non TSA-treated"/ {print}' s_Liver.txt|cut -f1|tr -d '"'|sed -re 's/\s+/_/g'|sort) <(ls liver/micro/|sed -re 's/\.CEL//g'|sort)|sed -re 's/$/.CEL/g' > liver_rat_nonTSA_files.txt
join -j 1 <(awk -F $'\t' '$1 ~ /R/ && $1 !~ /_W_/ && $3 !~ /HepG2\// && $3 ~ /"Primary rat hepatocytes TSA-treated"/ {print}' s_Liver.txt|cut -f1|tr -d '"'|sed -re 's/\s+/_/g'|sort) <(ls liver/micro/|sed -re 's/\.CEL//g'|sort)|sed -re 's/$/.CEL/g' > liver_rat_TSA_files.txt

R --file=normalize_carcinogenomic_liver.R

# Also normalize Carcinogenomics_extra arrays
R --file=normalize_carcinogenomics_extra.R
#+END_SRC

*** DONE Normalize HepG2 data from Magkoufopoulou et. al (GSE28878) :GSE28878:
- There are only single cell model

#+NAME: normalize_magkoufopoulou_data
#+HEADERS: :var data_dir="/home/jbayjanov/projects/tgx/dixa_classification/data/raw/" :prologue "#!/bin/bash" :eval no
#+BEGIN_SRC shell
cd ${data_dir}
mv hepg2 magkoufopoulou_hepg2
cd magkoufopoulou_hepg2
R --file=normalize_GSE28878_magkoufopoulou_HepG2_liver.R
#+END_SRC

*** DONE Normalize Predictomics data
- There are two different cell models: i) Primary hepatocytes; ii) HepG2
- These arrays use the custom cdf format: hgu133a2hsensgcdf
#+NAME: normalize_predictomics_data
#+HEADERS: :var data_dir="/home/jbayjanov/projects/tgx/dixa_classification/data/raw/predictomics" :prologue "#!/bin/bash" :eval no
#+BEGIN_SRC shell
cd ${data_dir}
# As the next command shows there are two cell models
# cut -f3 s_Predictomics.txt |sort|uniq -c
#      1 "Characteristics[Strain]"
#     66 "HepG2"
#     15 "PHH (BH-609)"
awk -F $'\t' '$1 ~ /Hep/' s_Predictomics.txt|cut -f1|tr -d '"' |cut -f1 -d'_'|sort|uniq|xargs -I{} find . -name "{}*.CEL"|cut -f2 -d'/' > hepg2_array_files.txt
awk -F $'\t' '$1 ~ /PHH/' s_Predictomics.txt|cut -f1|tr -d '"' |cut -f1 -d'_'|sort|uniq|xargs -I{} find . -name "{}*.CEL"|cut -f2 -d'/' > phh_array_files.txt
R --file=normalize_predictomics.R
#+END_SRC

*** DONE Normalize Deferme data
- Data is at: /ngs-data/data_storage/metadata/Deferme/GSE58235_RAW
- Use the file: GSE58235-GPL15798_series_matrix.txt
- These arrays use hthgu133pluspmhsensgcdf cdf package
- Normalize also 3 compounds Lize data separately: /ngs-data/data_storage/transcriptomics/microarray/mrna/Deferme/lize_3compounds_archive
- Lize 3 compound data use the same cdf: hthgu133pluspmhsensgcdf
#+NAME: normalize_deferme_arrays
#+HEADERS: :var data_dir="/home/jbayjanov/projects/tgx/dixa_classification/data/raw/deferme" :prologue "#!/bin/bash" :eval no
#+BEGIN_SRC shell
cd ${data_dir}
# In principle, next step is not necessary if these files were not gzipped. Because I can't gunzip it in its
# current location I have to copy them.
rsync -auvz /ngs-data/data_storage/transcriptomics/microarray/mrna/Deferme/GSE58235_RAW .
cd GSE58235_RAW
gunzip *.gz
cd ../
# The next step is necessary, because it also contains data from Magkoufopoulou et. al, which were already normalized see :GSE28878:
grep geo_access GSE58235-GPL15798_series_matrix.txt |tail -1|cut -f2-|tr -d '"'|tr '\t' '\n'|xargs -I{} find ./GSE58235_RAW/ -name "{}*.CEL" > array_files_of_GPL15798.txt
R --file=normalize_deferme_liver.R
# Copy Lize files, because these files have parenthesis in their names, which is not good
# After copying I change the parenthesis to underscores
rsync -auvz /ngs-data/data_storage/transcriptomics/microarray/mrna/Deferme/lize_3compounds_archive .
cd lise_3compounds_archive
rm *.JPG
rename 's/\(/_/g; s/\)/_/g' *.CEL
ln -s /ngs-data/data_storage/metadata/Deferme/lize_3compounds_archive/*.txt .
cd ../
R --file=normalize_deferme_lize3compounds.R
#+END_SRC

*** DONE Normalize Esnats data
- There is only a single cell model: hSKP-HPC

#+NAME: normalize_esnats_data
#+HEADERS: :var data_dir="/home/jbayjanov/projects/tgx/dixa_classification/data/raw/esnats" :prologue "#!/bin/bash" :eval no
#+BEGIN_SRC shell
cd ${data_dir}
R --file=normalize_esnats_arrays.R
#+END_SRC

*** DONE Normalize NTC human liver data
- There is only a single cell model: HepG2

#+NAME: normalize_esnats_data
#+HEADERS: :var data_dir="/home/jbayjanov/projects/tgx/dixa_classification/data/raw/ntc/" :prologue "#!/bin/bash" :eval no
#+BEGIN_SRC shell
cd ${data_dir}
ln -s /ngs-data/data_storage/transcriptomics/microarray/mrna/NTC/NTC_WP4.1.3_E02 .
R --file=normalize_ntc_human_liver.R
#+END_SRC

*** DONE Normalize TG-GATES data
- There is only a single cell model: hSKP-HPC

#+NAME: normalize_tg_gates_dixa006
#+HEADERS: :var data_dir="/home/jbayjanov/projects/tgx/dixa_classification/data/raw/tg_gates/" :prologue "#!/bin/bash" :eval no
#+BEGIN_SRC shell
cd ${data_dir}
# The link shown in the next row contains incomplete set of data. In total should be 2605, but it contains 2004 CEL files
# wget --mirror ftp://ftp.ebi.ac.uk/pub/databases/microarray/data/dixa/TG-Gates/archive/DIXA-006/
# mv ftp.ebi.ac.uk/pub/databases/microarray/data/dixa/TG-Gates/archive/DIXA-006 ./dixa_006
# rm -rf ./ftp.ebi.ac.uk/
cd ../
mv tg_gates tg_gates_old
mkdir tg_gates
cd tg_gates
cp ../tg_gates_old/*.txt .
wget -m ftp://ftp.biosciencedbc.jp/archive/open-tggates/LATEST/Human/in_vitro/
mv ./ftp.biosciencedbc.jp/archive/open-tggates/LATEST/Human/in_vitro .
rm -rf ./ftp.biosciencedbc.jp/
cd in_vitro
for i in $(ls *.zip); do unzip $i; done
find . -iname '*.cel' |sort > tg_gates_2605array_files.txt
R --file=normalize_tg_gates_2605arrays.R
#+END_SRC

*** DONE Normalize Ketelslegers data
- There is only a single cell model: HepaRG

#+NAME: normalize_ketelslegers_data
#+HEADERS: :var data_dir="/home/jbayjanov/projects/tgx/dixa_classification/data/raw/ketelslegers/" :prologue "#!/bin/bash" :eval no
#+BEGIN_SRC shell
cd ${data_dir}
mkdir -pv ../../processed/normalized/ketelslegers/liver
for i in $(ls *.zip); do unzip $i; done

R --file=normalize_ketelsregels_arrays.R
#+END_SRC


** DONE Maybe check all array files with md5sum to see if there are identical files

#+NAME: compare_md5sums
#+HEADERS: :var data_dir="/home/jbayjanov/projects/tgx/dixa_classification/data/raw/" :prologue "#!/bin/bash" :eval no
#+BEGIN_SRC shell
cd ${data_dir}
find . -iname '*.cel'|xargs -I{} md5sum {}|tee create_md5sums_for_all_arrays.txt
grep -v tg_gates_old create_md5sums_for_all_arrays.txt|cut -f1 -d' '  |sort|uniq -c|sed -re 's/^\s*//g'|sort -nr|wc -l # -> 0; So, this means no duplicate file
# There are NO DUPLICATE *.CEL files
#+END_SRC

* DONE Compare common genes between all array data and genes present in TK6 data [5/5]
** DONE Get common genes among all arrays we normalized

#+NAME: common_genes_in_arrays
#+HEADERS: :eval no 
#+BEGIN_SRC shell
cd /home/jbayjanov/projects/tgx/dixa_classification/data/processed/normalized
find . -iname '*.tsv'|xargs -I{} cut -f1 {}|grep -E '^ENSG[0-9]+'|sort |uniq -c|sed -re 's/^\s+//g'|sort -nr |sed -re 's/\s+/\t/g' > ensg_ids_study_count_14experiments.tsv
cut -f1 ensg_ids_study_count_14experiments.tsv |sort|uniq -c                                                                                                 
#     54 10
#   8192 12
#  11810 14
#      1 2
# The previous command shows that we can use at most 11810 if we want to use all experiments. If it is fine to use only 12 experiments
# then we can use 11810+8192 genes in the classification task
#+END_SRC

** DONE Create a table with all needed information for classification task
- The following 4 files were corrected/modified by Danyel by adding missing info:
# -rw-rw-r-- 1 jbayjanov jbayjanov 312K okt 26 19:16 classif_info_for_tg_gates.tsv
# -rw-rw-r-- 1 jbayjanov jbayjanov  23K okt 26 19:16 classif_info_for_ntc_wp4_1_3.tsv
# -rw-rw-r-- 1 jbayjanov jbayjanov  60K okt 26 19:16 classif_info_for_magkoufopoulou.tsv
# -rw-rw-r-- 1 jbayjanov jbayjanov  15K okt 26 19:16 classif_info_for_deferme_lize.tsv

#+NAME: classification_data_carcinogenomics
#+HEADERS: :var data_dir="/home/jbayjanov/projects/tgx/dixa_classification/data/raw/" :prologue "#!/bin/bash" :eval no
#+BEGIN_SRC shell
cd /home/jbayjanov/projects/tgx/dixa_classification/data/processed/normalized/carcinogenomics/liver
# Some file names contain spaces in a_liver* file.
# --> This results in 0 and that indicates there are no non-matching file names
join -o 1.1 2.1 -e HEYY -a 2 -j 1 <(cut -f16 ../../../../raw/carcinogenomics/a_liver_transcription_profiling_DNA_microarray.txt |tr -d '"'|tr ' ' '_'|sort) <(head -n1 -q h*|sed -re 's/\s+/\n/g'|grep -Ev '^\s*$'|sort)|grep HEYY|wc -l
# I did NOT use the following approach, because there were some errors in a_* file of carcinogenomics data
# cd /home/jbayjanov/projects/tgx/dixa_classification/data/raw/carcinogenomics
# paste  <(cut -f1,3,7,24,28,30,32,33,34,42 s_Liver.txt |head -1) <(cut -f4,6,10,16 a_liver_transcription_profiling_DNA_microarray.txt|head -1)|sed -re 's/ /_/g' > classification_relevant_info_liver.tsv && join -t $'\t' -j 1  <(grep -vE '"Rat' s_Liver.txt|cut -f1,3,7,24,28,30,32,33,34,42|sort -k 1) <(grep -vE '"Rat' a_liver_transcription_profiling_DNA_microarray.txt|cut -f1,4,6,10,16|sort -k 1) |sed -re 's/ /_/g' >> classification_relevant_info_liver.tsv 

## CHECK for this. It looks like file names are not matching
# grep -A 1 -B 1 BPI-T2-2NF5-24h-R-1-1 a_liver_transcription_profiling_DNA_microarray.txt |sed -re 'p; c \ '|less  <---- Indeed this is an error in the a* file
cd /home/jbayjanov/projects/tgx/dixa_classification/data/processed/normalized/carcinogenomics/liver 
# head -n1 ../../../../raw/carcinogenomics/classification_relevant_info_liver.tsv > classif_info_for_normalized_data.tsv && for i in $(head -n1 -q h*|sed -re 's/\s+/\n/g'|grep -Ev '^\s*$' |sort); do grep $i ../../../../raw/carcinogenomics/classification_relevant_info_liver.tsv; done >> classif_info_for_normalized_data.tsv

head -n1 ../../../../raw/carcinogenomics/s_Liver.txt|awk -F $'\t' '{print($0"\t\"array_file\"")}'|cut -f1,3,24,28,30,31,32,33,34,42,53 > classif_info_for_normalized_data.tsv && for i in $(head -n1 -q h*|sed -re 's/\s+/\n/g; s/\.CEL//g'|grep -Ev '^\s*$' |sort); do awk -F $'\t' -v s=$i 's ~ /_/{split(s,a,"_"); if($1 ~ a[1] && $1 ~ a[2]){print($0"\t\""s".CEL\"");}}; s !~ /_/ && $1 ~ s {print($0"\t\""s".CEL\"");}'   ../../../../raw/carcinogenomics/s_Liver.txt|cut -f1,3,24,28,30,31,32,33,34,42,53; done >> classif_info_for_normalized_data.tsv


## For carcinogenomics_extra there is not enough information
#+END_SRC

#+NAME: classification_data_deferme
#+HEADERS: :var data_dir="/home/jbayjanov/projects/tgx/dixa_classification/data/processed/normalized/deferme_hepg2/liver/" :prologue "#!/bin/bash" :eval no
#+BEGIN_SRC shell
cd ${data_dir}
cd ../../../../raw/deferme/lize_3compounds_archive/
# Convert the MS files to unix files; otherwise awk comparison doesn't work properly
dos2unix -n s_Lize_mena_Hepg2.txt s_Lize_mena_Hepg2_unix.txt 
dos2unix -n a_lize_mena_hepg2_transcription\ profiling_DNA\ microarray-1.txt a_lize_mena_hepg2_transcription_profiling_DNA_microarray-1_unix.txt 
dos2unix -n i_Investigation.txt i_Investigation_unix.txt 
# Remove symbolink links to original files
ls *.txt|grep -v _unix.txt|xargs -I{} rm {}
cd /home/jbayjanov/projects/tgx/dixa_classification/data/processed/normalized/deferme_hepg2/liver
# I put all of these commands into a single bash script
# head -n1 ../../../../raw/deferme/lize_3compounds_archive/s_Lize_mena_Hepg2_unix.txt|awk -F $'\t' '{printf "%s", $1"\t"$3"\t"$24 "\t\"Comment [ChEMBL ID]\"\t"; for(i=29; i<34; i++) printf "%s", $i"\t"; printf "%s", $41"\t\"array_file\"\n"}' > classif_info_for_deferme_lize.tsv
./get_select_classify_info.sh
 
# The following command gets info needed for classification info
# TODO 
awk 'NR>29 && NR<70' GSE58235-GPL15798_series_matrix.txt |awk 'NR<15{print}; NR==17{print}; NR==19{print}; NR==21{print}; NR==25{print}; NR==27{print}; NR==37{print}; NR>39{print}'|datamash transpose|sed -re 's/\!Sample/Sample/g'|sed -re  '1{s/Sample_characteristics_ch1\tSample_molecule_ch1/Time_point_h\tSample_molecule_ch1/;}; 2~1{s/("time \(h\): )([0-9]+)(")/"\2"/g}' > relevant_info_GSE58235_GPL15798_series_matrix.tsv
#+END_SRC

#+NAME: classification_data_esnats
#+HEADERS: :var data_dir="/home/jbayjanov/projects/tgx/dixa_classification/data/processed/normalized/esnats/liver/" :prologue "#!/bin/bash" :eval no
#+BEGIN_SRC shell
cd /home/jbayjanov/projects/tgx/dixa_classification/data/raw/esnats/ukk4_archive
# Convert MS files to unix files
dos2unix *.txt
cd ${data_dir}
./select_classify_info_esnats.sh
#+END_SRC

#+NAME: classification_data_magkoufopoulou
#+HEADERS: :var data_dir="/ngs-data/data/dixa_classification/data/processed/normalized/magkoufopoulou_hepg2/liver/" :prologue "#!/bin/bash" :eval no
#+BEGIN_SRC shell
cd ${data_dir}
./select_classify_magkoufopoulou.sh
#+END_SRC

#+NAME: classification_data_predictomics
#+HEADERS: :var data_dir="/ngs-data/data/dixa_classification/data/processed/normalized/predictomics/liver/" :prologue "#!/bin/bash" :eval no
#+BEGIN_SRC shell
cd /home/jbayjanov/projects/tgx/dixa_classification/data/raw/predictomics
rename 's/\s+/_/g' *.txt
# Convert MS files to unix files
dos2unix *.txt
cd ${data_dir}
./select_classif_info_for_predictomics.sh
#+END_SRC

#+NAME: classification_data_tg_gates
#+HEADERS: :var data_dir="/ngs-data/data/dixa_classification/data/processed/normalized/tg_gates/liver/" :prologue "#!/bin/bash" :eval no
#+BEGIN_SRC shell
cd /home/jbayjanov/projects/tgx/dixa_classification/data/raw/tg_gates
rename 's/\s+/_/g' *.txt
# Convert MS files to unix files
dos2unix *.txt
cd ${data_dir}
./select_classif_info_for_tg_gates.sh
#+END_SRC

#+NAME: classification_data_ntc
#+HEADERS: :var data_dir="/ngs-data/data/dixa_classification/data/processed/normalized/ntc/liver/" :prologue "#!/bin/bash" :eval no
#+BEGIN_SRC shell
cd /ngs-data/data/dixa_classification/data/raw/ntc
cp /ngs-data/data_storage/metadata/NTC/NTC_WP4.1.3_E02/*.txt .
rename 's/\s+/_/g' *.txt
# Convert MS files to unix files
dos2unix *.txt
cd ${data_dir}
./select_classif_info_for_ntc_wp4_1_3.sh
#+END_SRC

#+NAME: classification_data_ketelslegers
#+HEADERS: :var data_dir="/ngs-data/data/dixa_classification/data/processed/normalized/ketelslegers/liver/" :prologue "#!/bin/bash" :eval no
#+BEGIN_SRC shell
cd ${data_dir}
./select_classif_info_ketelslegers.sh
cp classif_info_for_ketelslegers.tsv ../../../classify_tsv/
#+END_SRC

** DONE Make meta-data training tables same or similar in their format with one another

#+NAME: format_training_meta_data
#+HEADERS: :var data_dir="/home/jbayjanov/projects/tgx/dixa_classification/data/processed/" :prologue "#!/bin/bash" :eval no
#+BEGIN_SRC shell
cd ${data_dir}
mkdir training_meta_info
cd training_meta_info
echo "This folder contains all meta-data for training data from each project in separate tsv files" > README.txt
rsync -auvz ../classify_tsv/*.tsv .
sed -i.bak -re 's/"//g; s/\t\?\t/\tNA\t/g; s/\?\s+Genotoxic/Maybe_Genototxic/g; s/TNF\?\?/TNF__/g; s/\tµM\t/\tuM\t/g; s/cell line: HepG2/HepG2/g' *.tsv
# Then a lot manual work was done to get correct files
mv classif_info_for_normalized_data.tsv classif_info_for_carcinogenomics_data.tsv 
for i in $(cut -f2 classif_info_for_carcinogenomics_data.tsv|sort|uniq|grep -v Strain); do 
    echo $i; awk -F$'\t' -v s=$i 'NR==1{print}; NR>1{if($2==s){print}}' classif_info_for_carcinogenomics_data.tsv > classif_info_for_carcinogenomics_${i}_data.tsv; 
done
echo "I splitted the file classif_info_for_carcinogenomics_data.tsv into following 4 different files:" >> README.txt
ls classif_info_for_carcinogenomics_*ep*tsv|nl >> README.txt

#+END_SRC

** DONE Create train/test data for classification
- Create a log-ratio data and when storing array data use the sample_id as described in the next line
- SampleId should be of the following form: study01_chemblID_concentration_timepoint. (concentration should include millimolar/percent info as well)
- Test caret with carcinogenomics data first.
*** Remove all non-ENSG id rows from normalized array files

#+NAME:remove_non_ensg_rows
#+HEADERS: :var data_dir="/ngs-data/data/dixa_classification/data/processed/normalized/" :prologue "#!/bin/bash" :eval no
#+BEGIN_SRC shell
cd ${data_dir}
mkdir ../old_normalized_files/
for i in $(find . -iname '*normaliz*.tsv'|grep -v class); do 
    echo "rsync -auvz $i ../old_normalized_files/ && sed -i.with_at -n -re '1,1p; /ENSG/ {s/(ENSG[0-9]+)(_at)/\1/g; p}' $i" >> remove_nonENSG.txt; 
    rsync -auvz $i ../old_normalized_files/ && sed -i.with_at -n -re '1,1p; /ENSG/ {s/(ENSG[0-9]+)(_at)/\1/g; p}' $i; 
done
#+END_SRC

*** Create a log-ratio data for training data

#+NAME: log_ratio_and_average
#+HEADERS: :var data_dir="/home/jbayjanov/projects/tgx/dixa_classification/data/" :prologue "#!/bin/bash" :eval no
#+BEGIN_SRC shell
cd ${data_dir}
mkdir averaged_ratios
awk -F$'\t' -v s="--file=/home/jbayjanov/projects/tgx/dixa_classification/src/data/logratio.R --args " 'BEGIN{print("#!/bin/bash\n");}NR>1{print("R "s" "$4" " $5" " $6)}' ../processed/averaged_ratios/array_files_for_averaging.tsv > average_ratio.sh
# Then the most recent version of running all commands are in the file: commands_average_ratios.txt
# Copy the results also to ngs-data processed part
cd ../
rsync -auvz ./averaged_ratios/* ./processed/averaged_ratios/
#+END_SRC

*** Create training data in a required format for caret
#+NAME: caret_training_data
#+HEADERS: :var data_dir="/home/jbayjanov/projects/tgx/dixa_classification/data/processed/average_ratios/" :prologue "#!/bin/bash" :eval no
#+BEGIN_SRC shell
cd ${data_dir}
cat *.tsv|cut -f1|grep ENSG|sort|uniq -c|grep -E '\s*13\s+'|grep -Eo 'ENSG[0-9]+' > ensg_ids_common_in_13_studies.txt
cat /ngs-data/data/dixa_classification/data/processed/TK6_test_data/GSE5*.txt|cut -f1|grep ENSG|sort|uniq -c|grep -E '^\s*2\s+' |grep -Eo 'ENSG[0-9]+' > ensg_ids_common_in_2_TK6_studies.txt 
cat ensg_ids_common_in_13_studies.txt ensg_ids_common_in_2_TK6_studies.txt |grep ENSG|sort|uniq -c|grep -E '^\s*2\s+' | sort > ensg_ids_common_in_all_13_and_2TK6_studies.txt
cd ../
mkdir caret_train_data
cd caret_train_data
awk -F$'\t' -v s="--file=/home/jbayjanov/projects/tgx/dixa_classification/src/data/convert_to_caret_input_format.R --args " 'BEGIN{print("#!/bin/bash\n");}NR>1{print("R "s" "$4" " $5" " $6" " $7)}' ../averaged_ratios/array_files_for_caret_format.tsv > caret_formatting.sh
chmod +x caret_formatting.sh
 head -n1 caret_train_Aanjaag_Jennen_Study12_HepaRG.tsv > all_caret_train_data.tsv
./caret_formatting.sh| tee caret_formatting_run_log.txt
grep -Evh 'Class\s+ENSG' caret_*tsv >> all_caret_train_data.tsv
cd ../
mkdir caret_run
cd caret_run
cp ../caret_train_data/all_caret_train_data.tsv all_caret_train_data.tsv.orig
# Because there is no genotoxicity info for the following compound
grep -v  study06_CHEMBL123292 all_caret_train_data.tsv.orig > all_caret_train_data.tsv
#+END_SRC


#+NAME: install_caret_and_deps
#+HEADERS: :eval no 
#+BEGIN_SRC R
# The followings are what I did on ngs-calc
install.packages("caret")
install.packages("doMC")
install.packages("randomForest")
install.packages("pamr")
install.packages("kernlab")
install.packages("kknn")
install.packages("pls")
install.packages("fastAdaboost") # This I want to test
install.packages("xgboost") # Also for testing

install.packages(c("BradleyTerry2","e1071","earth","fastICA","gam","ipred","kernlab","klaR","MASS","ellipse","mda","mgcv","mlbench","MLmetrics","nnet","party","pls","pROC","proxy","randomForest","RANN","spls","subselect","pamr","superpc","Cubist","testthat"))
source("https://bioconductor.org/biocLite.R")
biocLite("BiocUpgrade")
biocLite("impute")

library("impute")
testing_t = t(testing[,2:ncol(testing)])
imputed_testing_t = impute.knn(testing_t)
testing_imputed = as.data.frame(cbind(as.character(testing[,1]),t(imputed_testing_t$data)))
write.table(testing_imputed, file="TK6_test_transposed_01112017_imputed.txt",sep="\t", quote=FALSE, col.names=NA)
# There was a typo, which is also there in the non-imputed version, so the next step corrects it
sed -i.bak -re 's/MMS\tHTX/MMS\tGTX/' TK6_test_transposed_01112017_imputed.txt
testing_imp1 <-read.table("TK6_test_transposed_01112017_imputed.txt",header = TRUE, sep = "\t", row.names = 1);
pred_val_imp1 <- predict(allModels,newdata=testing_imp1,type="prob");
write.table(pred_val_imp1,"validation_set_prediction_on_imputed_test.txt",sep = "\t", col.names=NA)

for (i in allModels){
    valPred <- predict(i, testing_imp1);
    cf<- confusionMatrix(valPred, testing_imp1$Class);
    write.table(as.matrix(cf, what = "xtabs"),paste(i$method,"_validation_performance.txt", sep = ""),sep="\t", append = TRUE)
    write.table(as.matrix(cf, what = "overall"),paste(i$method,"_validation_performance.txt", sep = ""),sep="\t", append = TRUE)
    write.table(as.matrix(cf, what = "classes"),paste(i$method,"_validation_performance.txt", sep = ""),sep="\t", append = TRUE)
}
set.seed(7)
modelglm<- train(Class~., data=training, method="glm", trControl=control);
# TODO: Maybe try univariate feature selection approaches

# The followings are what I did on ngs-ada
# wget https://cran.r-project.org/src/contrib/Archive/lubridate/lubridate_1.6.0.tar.gz
# R CMD INSTALL /tmp/Rtmp29UDg9/downloaded_packages/lubridate_1.6.0.tar.gz 
# And then within R
install.packages("caret")
install.packages(c("BradleyTerry2","e1071","earth","fastICA","gam","ipred","kernlab","klaR","MASS","ellipse","mda","mgcv","mlbench","MLmetrics","nnet","party","pls","pROC","proxy","randomForest","RANN","spls","subselect","pamr","superpc","Cubist","testthat"))

#+END_SRC

** DONE Get correct ensemble gene IDs for TK6

#+NAME:
#+HEADERS: :eval no 
#+BEGIN_SRC shell
cd ~/temp/embl/
cut -f6 GSE58431_annotation.txt |grep ENST|sort|uniq > enst_gse58431.txt
join -t $'\t' -1 1 -2 2 -o 1.1 2.1 2.2 <(sort enst_gse58431.txt) <(sort -k2 ~/Downloads/mart_export.txt) |cut -f2|sort|uniq > tk6_ensg_ids.txt
head -n1 GSE58431_annotation.txt|cut -f4,5,6 > missing_595_ENST.txt && for i in $(join -a 1 -e HEY -t $'\t' -1 1 -2 2 -o 1.1 2.1 2.2 <(sort enst_gse58431.txt) <(sort -k2 ~/Downloads/mart_export.txt) |grep HEY|cut -f1); do grep $i GSE58431_annotation.txt |cut -f4,5,6 >> missing_595_ENST.txt; done

#for i in $(awk -F$'\t' '$6==0' GSE58431_annotation.txt |awk -F$'\t' '$5!=0'|cut -f5|head); do grep -B 1 -E "locus_tag=\"${i}\"" Homo_sapiens.GRCh38.90.chromosome.*.dat; done

awk -F$'\t' '$6==0' GSE58431_annotation.txt |awk -F$'\t' '$5!=0'|cut -f5|sort|uniq > genes_noEnst_withGeneSymbol.txt

wget -m ftp://ftp.ensembl.org/pub/release-90/embl/homo_sapiens/
cd ./ftp.ensembl.org/pub/release-90/embl/homo_sapiens/
gunzip *.gz
for i in $(ls Homo_sapiens.GRCh38.90.chromosome.*.dat); do a=$(echo $i|sed -re 's/.dat$//g'); echo "ensg_locusTag_${a}.tsv"; awk '$0 ~ /gene=ENSG/ || $0 ~ /locus_tag="*/' $i |cut -f2 -d'='|tr -d '"'|sed -re 's/\.[0-9]+$//g'|awk '{a=$1; getline; print(a"\t"$1);}' > ensg_locusTag_${a}.tsv; done

cd ~/temp/embl/
join -1 1 -2 2 -t $'\t' ./genes_noEnst_withGeneSymbol.txt <(cat ./ftp.ensembl.org/pub/release-90/embl/homo_sapiens/ensg_locusTag_Homo_sapiens.GRCh38.90.chromosome.*.tsv|sort -k2) > geneSymbol_ENSG.tsv
cut -f2 geneSymbol_ENSG.tsv |paste -s - tk6_ensg_ids.txt |tr '\t' '\n'|sort|uniq > tk6_ensg_ids_all.txt
# Find ENSG ids for genes with ENST ids but missing in Ensembl DB. So, we use locus_tag
join -1 1 -2 2 -t $'\t' <(awk -F $'\t' '$2!=0{print($2)}' ~/temp/embl/missing_595_ENST.txt|awk 'NR>1'|sort|uniq) <(cat ./ftp.ensembl.org/pub/release-90/embl/homo_sapiens/ensg_locusTag_Homo_sapiens.GRCh38.90.chromosome.*.tsv|sort -k2) > ENSG_for_missing_595_ENST.txt
cat <(cut -f2 ENSG_for_missing_595_ENST.txt) tk6_ensg_ids_all.txt|sort|uniq > tk6_ensg_ids_all_noMissing506genes.txt 
head -1  GSE58431_annotation.txt > GSE58431_annotation_withENSG.txt
while read line; do a=$(echo ${line}|sed -re 's/\s+/\t/g'|cut -f1); b=$(echo ${line}|sed -re 's/\s+/\t/g'|cut -f2); awk -v a=${a} -v b=${b} -F $'\t' 'BEGIN{OFS="\t";}{if($5==a){print($1,$2,$3,$4,$5,$6,b);}}' GSE58431_annotation.txt ; done < geneSymbol_ENSG.tsv  >> GSE58431_annotation_withENSG.txt
join -t $'\t' -1 1 -2 2 <(sort enst_gse58431.txt) <(sort -k2 ./mart_export3.txt) > human_ensg_2_enst.tsv
while read line; do a=$(echo ${line}|sed -re 's/\s+/\t/g'|cut -f1); b=$(echo ${line}|sed -re 's/\s+/\t/g'|cut -f2); grep -E "\s${a}\s" GSE58431_annotation.txt|cut -f1-6|awk -v b=${b} -F $'\t' 'BEGIN{OFS="\t";}{print($1,$2,$3,$4,$5,$6,b);}'; done < human_ensg_2_enst.tsv >> GSE58431_annotation_withENSG.txt
## Apparently there were some errors with the generation of the file: geneSymbol_ENSG.tsv
## Because some locus tags are present multiple times and have different ensemble gene ids
## So, to solve that issue I used NCBI Gene DB by manually searching it
## These are 24 genes that are present at least twice. Occurrence is shown in the 1st column
#      2 CLRN1-AS1
#      2 DGCR5
#      2 GABARAPL3
#      2 GHRLOS
#      2 HAR1A
#      2 LINC01115
#      2 LINC01481
#      2 OR7E47P
#      2 POLR2J4
#      2 RAET1E-AS1
#      2 SCARNA6
#      2 SNORA22
#      2 SNORA46
#      2 SNORA58
#      2 SNORD22
#      2 TP73-AS1
#      3 SCARNA4
#      3 SNORA12
#      3 SNORA77
#      4 SNORA19
#      4 SNORA33
#      5 SCARNA17
#      7 SNORA75
#     11 SNORA26
echo -e "CLRN1-AS1\tENSG00000239265" > multi_occurring_locusTags.tsv
echo -e "DGCR5\tENSG00000283406" >> multi_occurring_locusTags.tsv
echo -e "GABARAPL3\tENSG00000279980" >> multi_occurring_locusTags.tsv # Actually this is annotated as a pseude-gene
echo -e "GHRLOS\tENSG00000240288" >> multi_occurring_locusTags.tsv
echo -e "HAR1A\tENSG00000225978" >> multi_occurring_locusTags.tsv # This is annotated as non-protein coding 
echo -e "LINC01115\tENSG00000237667" >> multi_occurring_locusTags.tsv
echo -e "LINC01481\tENSG00000257815" >> multi_occurring_locusTags.tsv
echo -e "OR7E47P\tENSG00000257542" >> multi_occurring_locusTags.tsv # Also pseudo-gene
echo -e "POLR2J4\tENSG00000214783" >> multi_occurring_locusTags.tsv # Pseudo-gene
echo -e "RAET1E-AS1\tENSG00000268592" >> multi_occurring_locusTags.tsv
echo -e "SCARNA6\tENSG00000251791" >> multi_occurring_locusTags.tsv
echo -e "SNORA22\tENSG00000206634" >> multi_occurring_locusTags.tsv
echo -e "SNORA46\tENSG00000207493" >> multi_occurring_locusTags.tsv
echo -e "SNORA58\tENSG00000249020" >> multi_occurring_locusTags.tsv
echo -e "SNORD22\tENSG00000277194" >> multi_occurring_locusTags.tsv
echo -e "TP73-AS1\tENSG00000227372" >> multi_occurring_locusTags.tsv
echo -e "SCARNA4\tENSG00000281394" >> multi_occurring_locusTags.tsv
echo -e "SNORA12\tENSG00000212464" >> multi_occurring_locusTags.tsv
echo -e "SNORA77\tENSG00000221643" >> multi_occurring_locusTags.tsv
echo -e "SNORA19\tENSG00000207468" >> multi_occurring_locusTags.tsv
echo -e "SNORA33\tENSG00000200534" >> multi_occurring_locusTags.tsv
echo -e "SCARNA17\tENSG00000267322" >> multi_occurring_locusTags.tsv
echo -e "SNORA75\tENSG00000206885" >> multi_occurring_locusTags.tsv
echo -e "SNORA26\tENSG00000212588" >> multi_occurring_locusTags.tsv
## Now correct that file
comm  -3 <(cut -f1 tmp5.txt|sort) <(cut -f1 GSE58431_annotation.txt|sort)|sed -re 's/\s+//g' > probes_with_ensg.txt
head -1  GSE58431_annotation.txt > tmp5.txt
while read line; do a=$(echo ${line}|sed -re 's/\s+/\t/g'|cut -f1); b=$(echo ${line}|sed -re 's/\s+/\t/g'|cut -f2); grep -E "\s${a}\s0\s" GSE58431_annotation.txt|cut -f1-6|awk -v b=${b} -F $'\t' 'BEGIN{OFS="\t";}{print($1,$2,$3,$4,$5,$6,b);}' ; done < geneSymbol_ENSG_noDoubleOccur.tsv  >> tmp5.txt 
while read line; do a=$(echo ${line}|sed -re 's/\s+/\t/g'|cut -f1); b=$(echo ${line}|sed -re 's/\s+/\t/g'|cut -f2); grep -E "\s${a}\s" GSE58431_annotation.txt|cut -f1-6|awk -v b=${b} -F $'\t' 'BEGIN{OFS="\t";}{print($1,$2,$3,$4,$5,$6,b);}'; done < human_ensg_2_enst.tsv >> tmp5.txt 
join -j 1 -t $'\t' probes_with_ensg.txt  <(sort -k1b,1 GSE58431_annotation.txt) >> tmp5.txt 
mv tmp5.txt GSE58431_annotation_withENSG.txt
rsync -uavz GSE58431_annotation_withENSG.txt   jbayjanov@ngs-ada.tgx.unimaas.nl:/ngs-data/data/dixa_classification/data/processed/normalized/
#+END_SRC


#+NAME: map_3126_agilentprobes_to_ensg
#+HEADERS: :eval no 
#+BEGIN_SRC shell
# Here I map additional Agilent probes from TK6 project to Ensemble gene IDs
cd /home/bayjan/temp/embl/gse
## R Code:
## biocLite("HsAgilentDesign026652.db")
## library("HsAgilentDesign026652.db")
## x <- HsAgilentDesign026652ENSEMBL
## write.table(file='~/temp/embl/probes_HsAgilentDesign026652ENSEMBL.txt',as.data.frame(x),col.names=T, row.names=F,quote=FALSE,sep="\t")
## 
join -j 1 -t $'\t' <(sort -k1b,1 probes_HsAgilentDesign026652ENSEMBL.txt) <(sort -k1b,1 Agilent_annotation_additional_probes.txt ) > ensg_for_1332probes_Agilent_annotation_additional_probes.txt
# The next command gets an ENSG id for a given Refseq ID
echo -e "probe_id\trefseq_id\tensg_id" > agilent_probe_2_ensg_refseq.tsv && while read line; do a=$(echo $line|sed -re 's/\s+/\t/g'|cut -f1); b=$(echo $line|sed -re 's/\s+/\t/g'|cut -f2); echo "https://www.ncbi.nlm.nih.gov/gene/?term=${b}"; c=$(curl -s -X GET "https://www.ncbi.nlm.nih.gov/gene/?term=${b}"|grep -Eo '>Ensembl:ENSG[0-9]+<'|tr -d '<>') ; echo "${line}= =${a}= =${b}=   =${c}="; echo -e "${a}\t${b}\t${c}" >> agilent_probe_2_ensg_refseq.tsv; done < <(join -j 1 -t $'\t' <(join  -j 1 -t $'\t' <(awk -F$'\t' 'BEGIN{OFS="\t";}NR>151 && $1 ~ /A_[0-9]+_/{print($1,$4,$5,$6,$7,$9);}' GSE58431_family.soft |sort -k1b,1|uniq ) <(cut -f1 ../Agilent_annotation_additional_probes.txt |sort|uniq)) ../not_found_additional_agilent_probes.txt |cut -f1,2|awk -F $'\t' '$2 ~ /.+/{print}')
# The next command gets an ENSG id for a given NCBI Gene DB ID
echo -e "probe_id\tncbi_gene_id\tensg_id" > agilent_probe_2_ensg_ncbiGeneDBid.tsv && while read line; do a=$(echo $line|sed -re 's/\s+/\t/g'|cut -f1); b=$(echo $line|sed -re 's/\s+/\t/g'|cut -f2); echo "https://www.ncbi.nlm.nih.gov/gene/?term=${b}"; c=$(curl -s -X GET "https://www.ncbi.nlm.nih.gov/gene/?term=${b}"|grep -Eo '>Ensembl:ENSG[0-9]+<'|tr -d '<>') ; echo "${line}= =${a}= =${b}=   =${c}="; echo -e "${a}\t${b}\t${c}" >> agilent_probe_2_ensg_ncbiGeneDBid.tsv; done < <(join -j 1 -t $'\t' <(join  -j 1 -t $'\t' <(awk -F$'\t' 'BEGIN{OFS="\t";}NR>151 && $1 ~ /A_[0-9]+_/{print($1,$4,$5,$6,$7,$9);}' GSE58431_family.soft |sort -k1b,1|uniq ) <(cut -f1 ../Agilent_annotation_additional_probes.txt |sort|uniq)) ../not_found_additional_agilent_probes.txt |cut -f1,2,4|awk -F $'\t' '$2 !~ /.+/ && $3 ~ /.+/{print}')
# Combine the ensg ids that were found based on the refseq id or NCBI gene id
cat agilent_probe_2_ensg_*|grep ENSG|sed -re 's/Ensembl://g'|awk 'BEGIN{print("probe_id\trefseq_or_gene_id\tensg_id");}{print}' > agilent_probe_2_ensg_refseq_gene.tsv

#+END_SRC

* DONE Classify using caret package in R [3/3]
** DONE Classify on training data
** DONE Test the prediction on the test data (TK6 data)
** DONE Use feature elimination and the classify using only best features

#+NAME: univariate_feature_elimination
#+HEADERS: :eval no 
#+BEGIN_SRC R
library("caret")
input<-read.table("all_caret_train_data.tsv",header = TRUE, sep = "\t", row.names = 1)
dim(input)
training <- input
rm(input)
# and other parts of jb_caret.R script
gam_scores_train=c()
for(i in 2:ncol(training)){
gam_scores_train=c(gam_scores_train,gamScores(training[,i],training[,1]));
}
anova_scores_train=c()
for(i in 2:ncol(training)){
anova_scores_train=c(anova_scores_train,anovaScores(training[,i],training[,1]));
}
all.equal(anova_scores_train,gam_scores_train) # This gives TRUE
modelglm_featSelect <- train(Class~., data=training[, c(1,which(gam_scores_train<0.05)+1)], method="glm", trControl=control)
valPred_glm_fs <- predict(modelglm_featSelect, testing_imp1)
cfglm_fs<- confusionMatrix(valPred_glm_fs, testing_imp1$Class)
write.table(as.matrix(cfglm, what = "xtabs"),paste(modelglm$method,"_featSelect_validation_performance.txt", sep = ""),sep="\t", append = TRUE)
write.table(as.matrix(cfglm, what = "overall"),paste(modelglm$method,"_featSelect_validation_performance.txt", sep = ""),sep="\t", append = TRUE)
write.table(as.matrix(cfglm, what = "classes"),paste(modelglm$method,"_featSelect_validation_performance.txt", sep = ""),sep="\t", append = TRUE)

# Now start caret with all models and with the selected features
training_featSelect = training[, c(1,which(gam_scores_train<0.05)+1)]
methods1 = c("pam","svmLinear","svmLinear2", "rf","knn","pls", "svmRadial", "gbm", "xgbLinear","adaboost")
allModels_featSelect=list()
for(m in methods1){
    set.seed(7)
    print(paste("Started to work on model ",m,sep=""));
    model1 <- train(Class~., data=training_featSelect, method=m, trControl=control)
    allModels_featSelect[[m]]=model1;
}
results_featSelect <- resamples(list(PAM=allModels_featSelect$pam, KNN=allModels_featSelect$knn, svmL1=allModels_featSelect$svmLinear, RF=allModels_featSelect$rf, svmL2=allModels_featSelect$svmLinear2, svmR=allModels_featSelect$svmRadial, pls=allModels_featSelect$pls, gbm=allModels_featSelect$gbm, xgbL=allModels_featSelect$xgbLinear))
sum_feat=summary(results_featSelect)                                                                                                                                                                         write.table(sep="\t",file="results_featSelect_summary_ROC.txt",sum_feat$statistics$ROC, col.names=NA)
write.table(sep="\t",file="results_featSelect_summary_sens.txt",sum_feat$statistics$Sens, col.names=NA)
write.table(sep="\t",file="results_featSelect_summary_spec.txt",sum_feat$statistics$Spec, col.names=NA)
pred_val_imp1_fs <- predict(allModels_featSelect,newdata=testing_imp1,type="prob")
write.table(pred_val_imp1_fs,"validation_set_prediction_on_imputed_test_featSelect.txt",sep = "\t", col.names=NA)

for (i in allModels_featSelect){
    valPred_imp_fs <- predict(i, testing_imp1);
    cf_imp_fs<- confusionMatrix(valPred_imp_fs, testing_imp1$Class);
    write.table(as.matrix(cf_imp_fs, what = "xtabs"),paste(i$method,"_validation_performance_imp_featSelect.txt", sep = ""),sep="\t", append = FALSE)
    write.table(as.matrix(cf_imp_fs, what = "overall"),paste(i$method,"_validation_performance_imp_featSelect.txt", sep = ""),sep="\t", append = TRUE)
    write.table(as.matrix(cf_imp_fs, what = "classes"),paste(i$method,"_validation_performance_imp_featSelect.txt", sep = ""),sep="\t", append = TRUE)
}
png(file = "bwplot_imp_fs.png", bg = "transparent")
bwplot(results_featSelect)
dev.off()
svmL2_imp_fs_varImp = varImp(allModels_featSelect$svmLinear2)$importance
dim(svmL2_imp_fs_varImp)
# [1] 3107    2
length(which(svmL2_imp_fs_varImp[,1]!=svmL2_imp_fs_varImp[,2]))
# [1] 0
length(which(svmL2_imp_fs_varImp[,1]!=0))
# [1] 3106
length(which(svmL2_imp_fs_varImp[,2]!=0))
# [1] 3106
# Get important features using different method
install.packages("rminer")


# Find feature importance using univariate test
gam_scores_test_fs=c()
for(i in 2:ncol(testing_imp1)){
    gam_scores_test_fs=c(gam_scores_test_fs,gamScores(testing_imp1[,i],testing_imp1[,1]));
}
svmL2_imp = varImp(allModels_featSelect$svmLinear2)$importance;


####
# > allModels_featSelect$svmLinear2$results
#   cost       ROC      Sens      Spec      ROCSD     SensSD     SpecSD
# 1 0.25 0.9331225 0.7904598 0.9312907 0.02525438 0.07457755 0.03970534
# 2 0.50 0.9331330 0.7904598 0.9312907 0.02526402 0.07457755 0.03970534
# 3 1.00 0.9331225 0.7904598 0.9312907 0.02525438 0.07457755 0.03970534
#
# > svmL2_train_203genes$results
#   cost       ROC      Sens      Spec      ROCSD     SensSD     SpecSD
# 1 0.25 0.7913503 0.6124521 0.8498765 0.06420188 0.09909963 0.05233779
# 2 0.50 0.7807904 0.6057088 0.8198092 0.06219110 0.07895627 0.06131046
# 3 1.00 0.7762164 0.6137548 0.8125028 0.06231767 0.08358926 0.05792013
#
# > svmL2_train_69genes$results                               
#   cost       ROC      Sens      Spec      ROCSD     SensSD     SpecSD
# 1 0.25 0.7681915 0.5003065 0.9154994 0.05864426 0.10048195 0.03409146
# 2 0.50 0.7676793 0.5012644 0.9106397 0.05882733 0.09051054 0.03196197
# 3 1.00 0.7671708 0.5115326 0.9112346 0.05892186 0.08781068 0.03284612
#
# > svmL2_train_26genes$results
#   cost       ROC      Sens      Spec      ROCSD     SensSD     SpecSD
# 1 0.25 0.7626779 0.3739464 0.9509877 0.05493789 0.09294077 0.02565052
# 2 0.50 0.7617313 0.3750958 0.9497531 0.05503500 0.09322220 0.02639576
# 3 1.00 0.7620140 0.3762069 0.9491358 0.05498332 0.09290538 0.02673850
#
# > svmL2_train_15genes$results
#   cost       ROC      Sens      Spec      ROCSD     SensSD     SpecSD
# 1 0.25 0.7628248 0.3561303 0.9454770 0.05035040 0.08465970 0.02942414
# 2 0.50 0.7623939 0.3560920 0.9448709 0.05060342 0.08308903 0.02883107
# 3 1.00 0.7624459 0.3550192 0.9448709 0.05036036 0.07995182 0.02883107

# All 3107 genes
# Confusion Matrix and Statistics
# 
#           Reference
# Prediction GTX NGTX
#       GTX   31    1
#       NGTX  20   31
#                                           
#                Accuracy : 0.747           
#                  95% CI : (0.6396, 0.8361)
#     No Information Rate : 0.6145          
#     P-Value [Acc > NIR] : 0.007697        
#                                           
#                   Kappa : 0.5192          
#  Mcnemar's Test P-Value : 8.568e-05       
#                                           
#             Sensitivity : 0.6078          
#             Specificity : 0.9688          
#          Pos Pred Value : 0.9688          
#          Neg Pred Value : 0.6078          
#              Prevalence : 0.6145          
#          Detection Rate : 0.3735          
#    Detection Prevalence : 0.3855          
#       Balanced Accuracy : 0.7883          
#                                           
#        'Positive' Class : GTX             
# 
#                                           
# 203 genes 
# cf_imp_fs_203genes
# Confusion Matrix and Statistics
# 
#           Reference
# Prediction GTX NGTX
#       GTX   19    7
#       NGTX  32   25
#                                           
#                Accuracy : 0.5301          
#                 95% CI : (0.4174, 0.6407)
#     No Information Rate : 0.6145          
#     P-Value [Acc > NIR] : 0.9534226       
#                                           
#                   Kappa : 0.1343          
#  Mcnemar's Test P-Value : 0.0001215       
#                                           
#             Sensitivity : 0.3725          
#             Specificity : 0.7812          
#          Pos Pred Value : 0.7308          
#          Neg Pred Value : 0.4386          
#              Prevalence : 0.6145          
#          Detection Rate : 0.2289          
#    Detection Prevalence : 0.3133          
#       Balanced Accuracy : 0.5769          
#                                           
#        'Positive' Class : GTX             
# 
# 
# cf_imp_fs_69genes
# Confusion Matrix and Statistics
# 
#           Reference
# Prediction GTX NGTX
#       GTX   28    2
#       NGTX  23   30
#                                           
#                Accuracy : 0.6988          
#                 95% CI : (0.5882, 0.7947)
#     No Information Rate : 0.6145          
#     P-Value [Acc > NIR] : 0.06969         
#                                           
#                   Kappa : 0.4335          
#  Mcnemar's Test P-Value : 6.334e-05       
#                                           
#             Sensitivity : 0.5490          
#             Specificity : 0.9375          
#          Pos Pred Value : 0.9333          
#          Neg Pred Value : 0.5660          
#              Prevalence : 0.6145          
#          Detection Rate : 0.3373          
#    Detection Prevalence : 0.3614          
#       Balanced Accuracy : 0.7433          
#                                           
#        'Positive' Class : GTX             
# 
# > cf_imp_fs_26genes
# Confusion Matrix and Statistics
# 
#           Reference
# Prediction GTX NGTX
#       GTX   18    1
#       NGTX  33   31
#                                           
#                Accuracy : 0.5904          
#                  95% CI : (0.4769, 0.6972)
#     No Information Rate : 0.6145          
#     P-Value [Acc > NIR] : 0.7154          
#                                           
#                   Kappa : 0.2712          
#  Mcnemar's Test P-Value : 1.058e-07       
#                                           
#             Sensitivity : 0.3529          
#             Specificity : 0.9688          
#          Pos Pred Value : 0.9474          
#          Neg Pred Value : 0.4844          
#              Prevalence : 0.6145          
#          Detection Rate : 0.2169          
#    Detection Prevalence : 0.2289          
#       Balanced Accuracy : 0.6608          
#                                           
#        'Positive' Class : GTX             
#                                           
# > cf_imp_fs_15genes
# Confusion Matrix and Statistics
# 
#           Reference
# Prediction GTX NGTX
#       GTX   22    1
#       NGTX  29   31
#                                           
#                Accuracy : 0.6386          
#                 95% CI : (0.5257, 0.7412)
#     No Information Rate : 0.6145          
#     P-Value [Acc > NIR] : 0.3706          
#                                           
#                   Kappa : 0.344           
#  Mcnemar's Test P-Value : 8.244e-07       
#                                           
#             Sensitivity : 0.4314          
#             Specificity : 0.9688          
#          Pos Pred Value : 0.9565          
#          Neg Pred Value : 0.5167          
#              Prevalence : 0.6145          
#          Detection Rate : 0.2651          
#    Detection Prevalence : 0.2771          
#       Balanced Accuracy : 0.7001          
#                                           
#        'Positive' Class : GTX             
# 
testing_imp1_in_vivo <-read.table("TK6_test_transposed_01112017_imputed_in_vivo.txt",header = TRUE, sep = "\t", row.names = 1);
# The following compounds do overlap, In total 57
test_train_overlap_imp_in_vivo=testing_imp1_in_vivo[c(which(rownames(testing_imp1_in_vivo) %in% c("Caffeine","Cisplatin","Ethanol_2","Ethanol_4","Etoposide","MMS")),33:nrow(testing_imp1_in_vivo)),]

valPred_imp_fs_Allgenes_test_train_overlap <- predict(allModels_featSelect$svmLinear2, test_train_overlap_imp_in_vivo)
cf_imp_fs_Allgenes_test_train_overlap <- confusionMatrix(valPred_imp_fs_Allgenes_test_train_overlap, test_train_overlap_imp_in_vivo$Class)
# > cf_imp_fs_Allgenes_test_train_overlap
# Confusion Matrix and Statistics
# 
#           Reference
# Prediction GTX NGTX
#       GTX   27    1
#       NGTX  13   16
#                                           
#                Accuracy : 0.7544          
#                  95% CI : (0.6224, 0.8587)
#     No Information Rate : 0.7018          
#     P-Value [Acc > NIR] : 0.237649        
#                                           
#                   Kappa : 0.5122          
#  Mcnemar's Test P-Value : 0.003283        
#                                           
#             Sensitivity : 0.6750          
#             Specificity : 0.9412          
#          Pos Pred Value : 0.9643          
#          Neg Pred Value : 0.5517          
#              Prevalence : 0.7018          
#          Detection Rate : 0.4737          
#    Detection Prevalence : 0.4912          
#       Balanced Accuracy : 0.8081          
#                                           
#        'Positive' Class : GTX             
# 

valPred_imp_fs_69genes_test_train_overlapt <- predict(svmL2_train_69genes, test_train_overlap_imp_in_vivo)
cf_imp_fs_69genes_test_train_overlap <- confusionMatrix(valPred_imp_fs_69genes_test_train_overlapt, test_train_overlap_imp_in_vivo$Class)
# > cf_imp_fs_69genes_test_train_overlap
# Confusion Matrix and Statistics
# 
#           Reference
# Prediction GTX NGTX
#       GTX   28    2
#       NGTX  12   15
#                                           
#                Accuracy : 0.7544          
#                  95% CI : (0.6224, 0.8587)
#     No Information Rate : 0.7018          
#     P-Value [Acc > NIR] : 0.23765         
#                                           
#                   Kappa : 0.4981          
#  Mcnemar's Test P-Value : 0.01616         
#                                           
#             Sensitivity : 0.7000          
#             Specificity : 0.8824          
#          Pos Pred Value : 0.9333          
#          Neg Pred Value : 0.5556          
#              Prevalence : 0.7018          
#          Detection Rate : 0.4912          
#    Detection Prevalence : 0.5263          
#       Balanced Accuracy : 0.7912          
#                                           
#        'Positive' Class : GTX             

valPred_imp_fs_203genes_test_train_overlap <- predict(svmL2_train_203genes, test_train_overlap_imp_in_vivo)
cf_imp_fs_203genes_test_train_overlap <- confusionMatrix(valPred_imp_fs_203genes_test_train_overlap, test_train_overlap_imp_in_vivo$Class)
# > cf_imp_fs_203genes_test_train_overlap
# Confusion Matrix and Statistics
# 
#           Reference
# Prediction GTX NGTX
#       GTX   17    7
#       NGTX  23   10
#                                           
#                Accuracy : 0.4737          
#                  95% CI : (0.3398, 0.6103)
#     No Information Rate : 0.7018          
#     P-Value [Acc > NIR] : 0.99991         
#                                           
#                   Kappa : 0.0104          
#  Mcnemar's Test P-Value : 0.00617         
#                                           
#             Sensitivity : 0.4250          
#             Specificity : 0.5882          
#          Pos Pred Value : 0.7083          
#          Neg Pred Value : 0.3030          
#              Prevalence : 0.7018          
#          Detection Rate : 0.2982          
#    Detection Prevalence : 0.4211          
#       Balanced Accuracy : 0.5066          
#                                           
#        'Positive' Class : GTX             
# 
valPred_imp_fs_26genes_test_train_overlap <- predict(svmL2_train_26genes, test_train_overlap_imp_in_vivo)
cf_imp_fs_26genes_test_train_overlap <- confusionMatrix(valPred_imp_fs_26genes_test_train_overlap, test_train_overlap_imp_in_vivo$Class)
# > cf_imp_fs_26genes_test_train_overlap
# Confusion Matrix and Statistics
# 
#           Reference
# Prediction GTX NGTX
#       GTX   18    1
#       NGTX  22   16
#                                           
#                Accuracy : 0.5965          
#                  95% CI : (0.4582, 0.7244)
#     No Information Rate : 0.7018          
#     P-Value [Acc > NIR] : 0.9673          
#                                           
#                   Kappa : 0.2887          
#  Mcnemar's Test P-Value : 3.042e-05       
#                                           
#             Sensitivity : 0.4500          
#             Specificity : 0.9412          
#          Pos Pred Value : 0.9474          
#          Neg Pred Value : 0.4211          
#              Prevalence : 0.7018          
#          Detection Rate : 0.3158          
#    Detection Prevalence : 0.3333          
#       Balanced Accuracy : 0.6956          
#                                           
#        'Positive' Class : GTX             
# 
valPred_imp_fs_15genes_test_train_overlap <- predict(svmL2_train_15genes, test_train_overlap_imp_in_vivo)
cf_imp_fs_15genes_test_train_overlap <- confusionMatrix(valPred_imp_fs_15genes_test_train_overlap, test_train_overlap_imp_in_vivo$Class)
# > cf_imp_fs_15genes_test_train_overlap
# Confusion Matrix and Statistics
# 
#           Reference
# Prediction GTX NGTX
#       GTX   22    1
#       NGTX  18   16
#                                          
#                Accuracy : 0.6667         
#                  95% CI : (0.5294, 0.786)
#     No Information Rate : 0.7018         
#     P-Value [Acc > NIR] : 0.7681565      
#                                          
#                   Kappa : 0.3815         
#  Mcnemar's Test P-Value : 0.0002419      
#                                          
#             Sensitivity : 0.5500         
#             Specificity : 0.9412         
#          Pos Pred Value : 0.9565         
#          Neg Pred Value : 0.4706         
#              Prevalence : 0.7018         
#          Detection Rate : 0.3860         
#    Detection Prevalence : 0.4035         
#       Balanced Accuracy : 0.7456         
#                                          
#        'Positive' Class : GTX            
                                                                                                  

## Now test it in all in-vivo data
valPred_imp_fs_Allgenes_test_imp_inVivo_noNA <- predict(allModels_featSelect$svmLinear2, na.omit(testing_imp1_in_vivo))
cf_imp_fs_Allgenes_test_imp_inVivo_noNA <- confusionMatrix(valPred_imp_fs_Allgenes_test_imp_inVivo_noNA, na.omit(testing_imp1_in_vivo)$Class)
# > cf_imp_fs_Allgenes_test_imp_inVivo_noNA
# Confusion Matrix and Statistics
# 
#           Reference
# Prediction GTX NGTX
#       GTX   30    2
#       NGTX  19   17
#                                           
#                Accuracy : 0.6912          
#                  95% CI : (0.5674, 0.7976)
#     No Information Rate : 0.7206          
#     P-Value [Acc > NIR] : 0.7536950       
#                                           
#                   Kappa : 0.398           
#  Mcnemar's Test P-Value : 0.0004803       
#                                           
#             Sensitivity : 0.6122          
#             Specificity : 0.8947          
#          Pos Pred Value : 0.9375          
#          Neg Pred Value : 0.4722          
#              Prevalence : 0.7206          
#          Detection Rate : 0.4412          
#    Detection Prevalence : 0.4706          
#       Balanced Accuracy : 0.7535          
#                                           
#        'Positive' Class : GTX             

valPred_imp_fs_203genes_test_imp_inVivo_noNA <- predict(svmL2_train_203genes, na.omit(testing_imp1_in_vivo))
cf_imp_fs_203genes_test_imp_inVivo_noNA <- confusionMatrix(valPred_imp_fs_203genes_test_imp_inVivo_noNA, na.omit(testing_imp1_in_vivo)$Class)
# > cf_imp_fs_203genes_test_imp_inVivo_noNA
# Confusion Matrix and Statistics
# 
#           Reference
# Prediction GTX NGTX
#       GTX   18    7
#       NGTX  31   12
#                                           
#                Accuracy : 0.4412          
#                  95% CI : (0.3208, 0.5668)
#     No Information Rate : 0.7206          
#     P-Value [Acc > NIR] : 0.9999996       
#                                           
#                   Kappa : -8e-04          
#  Mcnemar's Test P-Value : 0.0001907       
#                                           
#             Sensitivity : 0.3673          
#             Specificity : 0.6316          
#          Pos Pred Value : 0.7200          
#          Neg Pred Value : 0.2791          
#              Prevalence : 0.7206          
#          Detection Rate : 0.2647          
#    Detection Prevalence : 0.3676          
#       Balanced Accuracy : 0.4995          
#                                           
#        'Positive' Class : GTX             
 
valPred_imp_fs_69genes_test_imp_inVivo_noNA <- predict(svmL2_train_69genes, na.omit(testing_imp1_in_vivo))
cf_imp_fs_69genes_test_imp_inVivo_noNA <- confusionMatrix(valPred_imp_fs_69genes_test_imp_inVivo_noNA, na.omit(testing_imp1_in_vivo)$Class)
# > cf_imp_fs_69genes_test_imp_inVivo_noNA
# Confusion Matrix and Statistics
# 
#           Reference
# Prediction GTX NGTX
#       GTX   28    2
#       NGTX  21   17
#                                           
#                Accuracy : 0.6618          
#                  95% CI : (0.5368, 0.7721)
#     No Information Rate : 0.7206          
#     P-Value [Acc > NIR] : 0.8865683       
#                                           
#                   Kappa : 0.3569          
#  Mcnemar's Test P-Value : 0.0001746       
#                                           
#             Sensitivity : 0.5714          
#             Specificity : 0.8947          
#          Pos Pred Value : 0.9333          
#          Neg Pred Value : 0.4474          
#              Prevalence : 0.7206          
#          Detection Rate : 0.4118          
#    Detection Prevalence : 0.4412          
#       Balanced Accuracy : 0.7331          
#                                           
#        'Positive' Class : GTX             

valPred_imp_fs_26genes_test_imp_inVivo_noNA <- predict(svmL2_train_26genes, na.omit(testing_imp1_in_vivo))
cf_imp_fs_26genes_test_imp_inVivo_noNA <- confusionMatrix(valPred_imp_fs_26genes_test_imp_inVivo_noNA, na.omit(testing_imp1_in_vivo)$Class)
# > cf_imp_fs_26genes_test_imp_inVivo_noNA
# Confusion Matrix and Statistics
# 
#           Reference
# Prediction GTX NGTX
#       GTX   18    1
#       NGTX  31   18
#                                           
#                Accuracy : 0.5294          
#                  95% CI : (0.4045, 0.6517)
#     No Information Rate : 0.7206          
#     P-Value [Acc > NIR] : 0.9997          
#                                           
#                   Kappa : 0.2122          
#  Mcnemar's Test P-Value : 2.951e-07       
#                                           
#             Sensitivity : 0.3673          
#             Specificity : 0.9474          
#          Pos Pred Value : 0.9474          
#          Neg Pred Value : 0.3673          
#              Prevalence : 0.7206          
#          Detection Rate : 0.2647          
#    Detection Prevalence : 0.2794          
#       Balanced Accuracy : 0.6574          
#                                           
#        'Positive' Class : GTX             

valPred_imp_fs_15genes_test_imp_inVivo_noNA <- predict(svmL2_train_15genes, na.omit(testing_imp1_in_vivo))
cf_imp_fs_15genes_test_imp_inVivo_noNA <- confusionMatrix(valPred_imp_fs_15genes_test_imp_inVivo_noNA, na.omit(testing_imp1_in_vivo)$Class)
# > cf_imp_fs_15genes_test_imp_inVivo_noNA
# Confusion Matrix and Statistics
# 
#           Reference
# Prediction GTX NGTX
#       GTX   22    1
#       NGTX  27   18
#                                           
#                Accuracy : 0.5882          
#                  95% CI : (0.4623, 0.7063)
#     No Information Rate : 0.7206          
#     P-Value [Acc > NIR] : 0.9935          
#                                           
#                   Kappa : 0.2793          
#  Mcnemar's Test P-Value : 2.306e-06       
#                                           
#             Sensitivity : 0.4490          
#             Specificity : 0.9474          
#          Pos Pred Value : 0.9565          
#          Neg Pred Value : 0.4000          
#              Prevalence : 0.7206          
#          Detection Rate : 0.3235          
#    Detection Prevalence : 0.3382          
#       Balanced Accuracy : 0.6982          
#                                           
#        'Positive' Class : GTX             
 
svmL2_test_train_overlap_models = list(svmL2_all=allModels_featSelect$svmLinear2, svmL2_203genes=svmL2_train_203genes, svmL2_69genes=svmL2_train_69genes, svmL2_26genes=svmL2_train_26genes, svmL2_15genes=svmL2_train_15genes)
pred_val_svmL2_test_train_overlap <- predict(svmL2_test_train_overlap_models,newdata=test_train_overlap_imp_in_vivo,type="prob")

# I saved the R commands history
savehistory(file = "last_100K_R_commands.txt")
# I also saved  allModels_featSelect, did not save allModels because it was way too big
save(allModels_featSelect,file="allModels_featSelect.RData")
#+END_SRC


#+NAME: copy_ngs_calc_dixa_classif
#+HEADERS: :var data_dir="/home/jbayjanov/projects/tgx/" :prologue "#!/bin/bash" :eval no
#+BEGIN_SRC shell
# This is done on ngs-calc
cd ${data_dir}
rsync -auvz dixa_classification_temp /share/data/openrisknet/
# I will NO longer on ~/projects/tgx/dixa_classification_temp, but will work on /share/data/openrisknet/dixa_classification_temp.
# I may delete ~/projects/tgx/dixa_classification_temp in the future
#+END_SRC

*** Correct TK6 in-vitro genotoxicity info to in-vivo genotoxicity info

#+NAME: tk6_in_vivo_genotoxicity
#+HEADERS: :var data_dir="/home/jbayjanov/projects/tgx/dixa_classification_temp/caret_run/" :prologue "#!/bin/bash" :eval no
#+BEGIN_SRC shell
# This is on ngs-calc
cd ${data_dir}
sed -r -f TK6_in_vitro_to_in_vivo.sed   TK6_test_transposed_01112017_imputed.txt > TK6_test_transposed_01112017_imputed_in_vivo.txt

#+END_SRC

* Download data from the following sources
** DiXa data warehouse: 
URL: http://wwwdev.ebi.ac.uk/fg/dixa/browsestudies.html
** GEO data
** ArrayExpress
** TK6 data
These were provided by Danyel:
- https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE58431
- http://www.sciencedirect.com/science/article/pii/S2352340915001699
- https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE51175
- http://onlinelibrary.wiley.com/doi/10.1002/em.21941/epdf
First I downloaded their RAW data and then also their normalized data
** HepG2 (TGX data)
- https://academic.oup.com/carcin/article-lookup/doi/10.1093/carcin/bgs182
- https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE28878

projects/tgx/dixa_classification/notebooks/
* Meeting 27 Nov 2017
** Test on multi-species model
Rat
Human
Mouse
Use also orthology information not only array data
** Kick out TK6
** Time-line
* Meeting 11 Jan 2018
** Data
Only use orthologous genes that are present in all 3 species
Try classification analysis on each species separately
Divide the data into training and test sets

** Time-line
